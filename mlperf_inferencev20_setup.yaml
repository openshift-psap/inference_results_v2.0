apiVersion: v1
kind: Pod
metadata:
  name: mlinferencesetup
  namespace: test
spec:
  restartPolicy: OnFailure
  containers:
    - name: inference
      image: "quay.io/dfeddema/mlperfinferencev20:1.0.0"
      command: ["/bin/bash", "-ec", "cd /work; tail -f /dev/null " ]
      runAsUser: 1000
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: all
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: NVIDIA_REQUIRE_CUDA
          value: "cuda>=5.0"
      resources:
        limits:
          nvidia.com/gpu: 1 # requesting 1 GPU
      securityContext:
        privileged: true
      volumeMounts:
      - mountPath: /work
        name: test-volume
      - mountPath: /data
        name: test-volume2
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /root/inference_results_v2.0/closed/NVIDIA 
  - name: test-volume2
    hostPath:
      # directory location on host
      path: /data
